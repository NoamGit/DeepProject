%
% File acl2016.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color,soul}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Local Entity Disambiguation for Web Fragments using Attentional RNNs}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Hello, My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who?
My name is. what? my name is. who? Lorem ipsum.
\end{abstract}

\section{Introduction}

Named Entity Disambiguation (NED) is the task of disambiguating mentions within a fragment of text against a given knowledge base of entities, and linking each mention to its intended entity. It has long been recognized as an important downstream task for many NLP problems such as text categorization \cite{gabrilovich2007computing} and information retrieval \cite{dalton2014entity}. 

NED algorithms can be broadly divided into local and global approaches where local algorithms disambiguate one mention at a time and typically rely only on a short local context of the given mention while global algorithms assume the underlying text is coherent and exploit this fact to simultaneously disambiguate multiple mentions within a single document using a coherency model. Much attention was given recently to global algorithms \cite{ratinov2011local,guo2014entity,pershina2015personalized} \hl{<- there's a ton of newer stuff} which demonstrate very good performance in many cases, however these are inherently optimized for long and coherent documents. In many important cases however, data is not presented as clean and coherent, but rather is given as short and noisy fragments. For example, short textual fragments taken from web pages, tweets and other social media data can be too short for global approaches and are arguably very noisy and not very coherent. 

Recent applications of Deep Learning have demonstrated Deep Neural Network (DNN) architectures can achieve state of the art results on a wide variety of tasks from image classification \cite{krizhevsky2012imagenet} to machine translation \cite{bahdanau2014neural}. When provided with large training datasets and sufficient computational resources DNNs can learn powerful data representations and model complex feature interactions. We present a Deep Learning based local disambiguation algorithm where Deep Attentional Recurrent Neural Networks (RNN) are trained to rank entities based on a noisy context using large quantities of crowd-sourced data. RNNs are powerful language modeling architectures which we supplement with an attention mechanism that allows the model to learn which parts of context are useful for disambiguation. We incorporate the trained neural model into a full NED system to demonstrate its performance.

We craft a large-scale dataset of short web fragments containing mentions disambiguated into Wikipedia by thousands of website editors. Our dataset is based on a subset of the Wikilinks dataset \cite{singh12:wiki-links} collected by Singh at el. at Google. We demonstrate our model greatly outperforms existing state-of-the-art NED algorithms on this large and noisy data. In addition we examine our algorithm on CoNLL \cite{hoffart2011robust}, a standard NED dataset and show results comparable to other state-out-the-art methods on a smaller and cleaner dataset.

\section{Methodology}

We describe our main component, a neural network used for modeling context and show how it can be integrated in a full disambiguation framework. Our main Attentional RNN component takes a context and a single candidate entity as input and computes a ranking score for the candidate given the context. \hl{rephrase, make the point we take a single candidate at a time, the overall framework will come later}

\subsection{Attentional RNN model}

We first focus on the mechanics of our Attentional RNN component as depicted in Figure 1. The Attentional RNN takes a window of words context as input and transforms it into a fixed length vector of features. It takes an embedded vector representation for a candidate entity to control its attention as well. This component is plugged into a larger DNN model and is trained by SGD and back-propagation along with the larger model. During the training process it learns to extract a useful feature representation of the context (given the candidate entity under consideration) for optimizing the loss function defined by the larger model. In our case predicting a ranking score for the candidate entity given the context.
The basic component of our model is a standard RNN unit to process the context input. In our implementation we used standard GRU units \cite{cho2014learning} as these are state-of-the-art RNN models but any other RNN unit can be used as a drop-in replacement. 

\begin{equation}
	\label{eq1}
	\begin{aligned}
		& h_t=f_{\Theta_1}(h_{t-1}, v_t) \\
		& o_t=g_{\Theta_2}(h_t)
	\end{aligned}
\end{equation}

Equation \ref{eq1} represents the general semantics of an RNN unit. An RNN reads a sequence of vectors $\{v_t\}$ and maintains a hidden state vector $\{h_t\}$. At each step a new hidden state is computed from the previous hidden state and the next input vector by a function $f$ parametrized by learnable parameters $\Theta_1$. The output at each step is computed from the hidden state using a function $g$ that is optionally parametrized by learnable parameters $\Theta_2$. 

A straight-forward use of an RNN in our model would be to train the RNN so that given a context as a sequence of word embedding vectors $\{v_t\}$, the last output $o_t$ will be used to predict the ranking score of the candidate entity directly. However ??, ?? have shown attentional models are beneficial for NED which motivated us to incorporate a simple neural attention model. The attention model takes the entire sequence of intermediate output vectors of the RNN $\{o_t\}$ and computes a weighted average of the vectors governed by the equations shown in equation \ref{eq2}

\begin{equation}
	\label{eq2}
	\begin{aligned}
	& a_t \in \mathbb{R}; a_t=r_{\Theta_3}(o_t, c) \\
	& a'_t  = \frac{1}{\sum_{i=1}^{t} \exp\{a_i\}} \exp \{a_t\} \\
	& o_{attn}=\sum_{i=1}^{t} a'_t o_t
	\end{aligned}
\end{equation}

The main component in equation \ref{eq2} is the function $r_{\Theta_3}$ which is parametrized by $\Theta_3$ and computes an attention value for each word based on a controller $c$ and the output of the RNN at step $t$. We then use a softmax to squeeze the attention values such that $\sum_{i=1}^{t} a'_i = 1$ and compute $o_{attn}$, the output of the attention model. The attention controller in our case is an embedding vector of the candidate entity itself. This allows the attention mechanism to decide how much individual words are important for ranking a given candidate. Our attention controller $c$ is an embedding vector of the candidate entity and our attention function $r$ is parametrized as a trainable single layer NN as shown in equation \ref{eq3}

\begin{equation}
\label{eq3}
r_{\Theta_3}(o_t, c) = Ao_t + Bc \\
\end{equation}

\subsection{Neural Network framework details and training regime \hl{AND MIXED NOTES}}
Our attentional RNN is incorporated in the NN framework as depicted in figure ???. We incorporate two separate RNNs, one for the left context and one for the right context. Note we feed the right context in reverse into RNN. Each RNN outputs a vector and these two vectors are concatenated along with the candidate entity embedding and are fed into a classifier DNN with a single hidden layer of size 300 and a relu non-linearity function, followed by an output layer with 2 neurons and a softmax non linearity. We train the neural network to optimize a cross entropy loss function where the first output neuron is set to predict the probability of the candidate entity being the correct one, and the second output neuron predicts the probability of the candidate entity not being the correct one. We have examined using a more conventional setting with a single output neuron trained using least squares loss, however this setting was found harder to train. We have implemented the entire model using the excellent Keras over Theano framework.

Due to framework limitations we have implemented our RNNs to take a fixed size left and right contexts. These where set a 20 word window to each side of the mention. When the context is shorter on either side, we pad it with a special $PAD$ symbol. We use a stop-word list to filter less informative stop-words.

Since our NN requires a pair of context and candidate entity as input, careful consideration was given to the training procedure. Following Mikolov at el. (?) we used a Negative Sampling approach where for each positive example the network is trained on, it is also trained on $k$ negative samples, where the context remains the same but the candidate is randomly sampled from all possible entities. We deviate from Mikolov where he samples according to the distribution of words in the text and in our case we have found uniform sampling to produce the best results. With uniform sampling the ration of positive to negative samples for each entity is proportional to the prior probability of the entity within the corpus, which bias the network toward ranking common entities higher.

During training our network is trained using error back propagation and SGD (No. Maybe ADAM?). We allow the error to propagate thou all part of the network and fine tune all trainable parameters, including fine-tuning the word embeddings.

\subsection{Training word and entity embeddings}
\hl{Make sure this part is indeed accurate and neccessary}

In this section we describe our methodology for pre-training a useful word and entity embeddings for incorporating in our model. Training our model implicitly trains its dictionaries of both word and entities by SGD and error back-propagation, however we have found that pre-trained embeddings improve model performance. To this end we have devised a Skip Gram with Negative Sampling (SGNS) \hl{ref} based training procedure that simultaneously trains both word and entity vectors in the same embedded space. We note Yamada at el. \hl{add ref} have shown a somewhat similar and very successful method. However while Yamada optimizes directly for disambiguation performance, we optimize for a simple, fast and plausible starting point for our deep model.

We use the word2vecf library by \hl{ref} that is derived from the original word2vec code \hl{ref} but allows to train directly on a dataset made of $(word,context)$ pairs rather then a textual corpus in a long string format. We exploit this and redefine the notion of $context$ from a context word to a context entity id. Specifically we take the corpus of Wikipedia pages as our knowledge base of entities and create a dataset where each page identified by the internal $pageid$ in Wikipedia database is scanned and split into a list of words ${word_i}$. We then add the pair $(word_i,pageid)$ for each word in each page in Wikipedia.

As shown by \hl{? and Levi, cite} training using SGNS on this dataset is produces word and entity embeddings that implicitly factorize the PPMI matrix of word-entity co-occurrence matrix. We note that this well-known matrix is closely related to the TFIDF word-entity matrix used by \hl{Gabrielovich and Markovich} in Explicit Semantic Analysis and was found useful in a wide array of NLP tasks. 

\hl{words are close to entities they are indicative of. Words appearing in similar entity pages are grouped together. Entities with similar pages are grouped together}

\hl{show the old analogies experiment we did indicating semantic structure}

\subsection{Using Deep Attentional RNN model with GBRT}

\hl{GBRT were used successfuly by Yamada and ?. State-of-the-art learning-to-rank algorithms. Incorporate our model with well established statistical features - Yamada base. Boosts results }

\section{Web-Fragment based NED Dataset}
We introduce a new large scale NED dataset for web-fragment crawled from the web. Our dataset is derived from the Wikilinks dataset originally collected at Google by \hl{??}  for a cross-document co-reference task. cross-document co-reference entails clustering mentions referring to the same entity across a set of documents without consulting a predefined knowledge base of entities, and is many-a-time regarded as a downstream task for knowledge base population. Wikilinks is constructed from crawling websites and collecting hyperlinks into Wikipedia and the web context they appear in. These hyperlinks are assumed to be ground-truths for linking mentions (hyperlink anchor-text) to distinct entities (Wikipedia pages). Wikilinks contains 40 million mentions covering 3 million entities collected from over 10 million web pages.

Despite the difference between a NED and a co-reference task, we believe the nature of Wikilinks makes it directly applicable to NED as well. It can be seen as a large-scale, naturally-occurring and crowd-sourced dataset where thousands of human annotators provide ground truth for mentions of interest. Its web sourcing entails an added interest since it is contains every kind of noise expected from automatically gathered web content. This includes on the one hand many faulty, misleading and peculiar ground truth labels, and on the other noisy, malformed and incoherent textual context for mentions. 

While noise in crowd-sourced data is a necessary trade-off for quantity, we believe the contextual noise is represents an interesting test case as most existing datasets for NED are sourced from mostly coherent and well formed text such as news articles and Wikipedia pages \hl{datasets: CoNLL, Tac, Wikipedia, ACE, etc}. Such a test case requires utilizing strong and adaptive local disambiguation techniques, and marginalizes the utility of coherency based global approaches.

\subsection{Preprocessing the Wikilinks dataset}

In order to utilize the Wikilinks dataset for NED we have first performed a number of filtering and preprocessing steps. Since we have opted for focusing on local disambiguation we choose a version of Wikilinks with only local contexts for mentions and URLs to the containing page rather then the much larger version with full pages\hl{link to download}. This sums to around 5Gb of compressed data (compared to 180Gb for the full texts).

The first preprocessing step we took was resolving ground-truth links using a \hl{fillin date} dump of the Wikipedia database. The same dump was consistently used throughout this research. We used the $page$ and $redirect$ tables for resolution and kept the database id as a unique identifier for Wikipedia pages (entities). We discarded mentions where the ground-truth could not be resolved. This resulted in retaining over \hl{97\%} of the mentions. We then permuted the order of mentions within the data and split it into train, evaluation and test set. We split the data $90\% / 10\% / 10\%$. Since websites might include duplicate or closely related content we did not assign mentions into splits on an individual base but rather collected all origin domains and assigned each domain along with all mentions collected from it into a split collectively.

The last and most important task was filtering the data. We first counted how many times each mention refers to each entity: $\#\{e|m\}$ and calculated the prior conditional-probability of an entity given a mention: $p(e|m)=\#\{e|m\}/\sum_{e'}\#\{e'|m\}$. Examining these distributions revealed most mentions either had very little ambiguity or appeared very few times and had a number of ground truths each appearing just a handful of times. We have deemed unambiguous mentions to be not-interesting and scarce mentions to be suspected as noise with a high probability and designed a procedure to filter both this cases. We therefore decided to retain only mentions for whom at least two ground-truth entities have $\#\{e|m\}\ge 10$ and $p(e|m)\ge0.1$. 

This procedure aggressively filtered the dataset and we were left with \hl{?} mentions. We believe that doing so filters uninteresting cases while emitting a dataset that is large scale yet manageable in size for research purposes. We note that we have considered filtering $(mention,entity)$ pairs where $\#\{entity|mention\}\le 3$ since these can also be suspected as noise however against this procedure as this might filter out long tail entities, a case which was deemed interesting in previous art.

\bibliographystyle{acl2016}
\bibliography{_our_submission}

\end{document}
