%
% File acl2016.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color,soul}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{using Attentional RNNs for Local Named Entity Disambiguation}

\author{First Author \\
	Affiliation / Address line 1 \\
	Affiliation / Address line 2 \\
	Affiliation / Address line 3 \\
	{\tt email@domain} \\\And
	Second Author \\
	Affiliation / Address line 1 \\
	Affiliation / Address line 2 \\
	Affiliation / Address line 3 \\
	{\tt email@domain} \\}

\date{}

\begin{document}
	\maketitle
	\begin{abstract}
		Hello, My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who?
		My name is. what? my name is. who? Lorem ipsum.
	\end{abstract}
	
	\section{Introduction}
	
	Named Entity Disambiguation (NED) is the task of disambiguating mentions within a fragment of text against a given knowledge base of entities and linking each mention to its intended entity. It has been recognized as an important downstream task for many NLP problems such as text categorization \cite{gabrilovich2007computing} and information retrieval \cite{dalton2014entity}. 
	
	NED algorithms can broadly be divided into local and global approaches, where local algorithms disambiguate mentions independently using local textual context while global approaches assume some coherence among mentions within a single document and therefore employ a coherency model to simultaneously disambiguate many mentions. Much attention was given recently to global algorithms \cite{ratinov2011local,guo2014entity,pershina2015personalized} \hl{(there's newer stuff)} which demonstrate very good performance on standard datasets. However while most standard datasets are based on text corpora such as news reports and Wikipedia paragraphs that are expected to be relatively long, well formed and coherent, other corpora such as fragments of web pages can be shorter, noisier and less coherent.
	
	Deep Neural Networks (DNN) have recently gained traction as state-of-the-art architectures that can model powerful data representations and complex feature interaction. DNNs achieve state-of-the-art results on a wide variety of tasks ranging from image classification \cite{krizhevsky2012imagenet} to machine translation \cite{bahdanau2014neural}. He at el. \cite{he2013learning} used stacked auto-encoders to learn entity and context representations for NED. More recently Sun at el. and Francis-Landau at el. \cite{sun2015modeling,francis2016capturing} have proposed models that incorporate convolutional neural networks for modeling and combining representations for a range of input signals and granularities. 
	
	We propose a novel Deep-Learning based disambiguation algorithm where Recurrent Neural Networks (RNNs) with an attention mechanism are employed to model local context and discriminate correct entity assignments from corrupt ones. \hl{differentiate RNN from CNN. Why are their better?} Recurrent Neural Networks are state-of-the-art language modeling architectures that are designed to model sequential data such as text and have been demonstrated to give state-of-the-art results for NLP related tasks such as Neural Machine Translation \cite{bahdanau2014neural} and Language Modeling \hl{? cite:??}. Attention mechanisms are natural extensions for RNNs where the model is trained to focus on the most relevant parts of the input text. Attention mechanisms have been successfully employed for tasks such as Neural Machine Translation \cite{bahdanau2014neural} and Parsing \cite{vinyals2015grammar}. 
	
	We present a novel word2vec \cite{mikolov2013distributed} based method for initializing word and entity embeddings employed by the model, and demonstrate its importance. While training our model we further fine-tune these embeddings via gradient descent and back-propagation.
		
	We craft a large-scale dataset of short web fragments containing mentions disambiguated into Wikipedia by thousands of website editors. Our dataset is based on a subset of the Wikilinks dataset \cite{singh12:wiki-links} which is re-purposed for NED. It represents a large-scale and difficult task with \hl{count} unique mentions disambiguated into $100K$ unique entities and where context is limited and noisy. We demonstrate our model greatly outperforms existing state-of-the-art NED algorithms on this dataset and in addition examine our algorithm on CoNLL \cite{hoffart2011robust}, a standard NED dataset, and show results comparable to other state-out-the-art methods on a smaller and cleaner dataset.
	
	\section{Methodology}
	
	The architecture for our model is presented in \hl{Figure 1}. The model takes a window-of-words to the left and right of a mention as its textual context and some candidate entity. The left and right contexts are fed into a duo of Attentional RNNs which process each side and produce a fixed length vector representation. Notice the right side is fed backwards into the model. Each Attentional RNN also uses the candidate entity to control its attention which allows it to attend to the most discriminating parts of the context given the candidate at hand. 
	
	The output vectors generated by both Attentional RNNs and the embedding of the entity itself are then fed into a classifier network consisting of a hidden layer, and an output layer with two output units and softmax activation. The output units are trained to emit the likelihood of the candidate being a correct or corrupt assignment. We optimize our model end-to-end with a cross-entropy loss function. 
	
	We have set the hidden layer size to be 300 and used a ReLU activation for this layer. We have found the width and depth of the classifier to be of little impact on performance, but using a ReLU non-linearity was found to be important. We have also applied dropout with $p=0.5$ to the hidden layer.

	\subsection{Attentional RNN component}
	
	The mechanics of our Attentional RNN component are depicted in \hl{Figure 2}. In our implementation we used a standard GRU unit \cite{cho2014learning} as it is a state-of-the-art RNN model but any other RNN unit can be used as a drop-in replacement. 
		
	Equation \ref{eq1} represents the general semantics of an RNN unit. An RNN reads a sequence of vectors $\{v_t\}$ and maintains a hidden state vector $\{h_t\}$. At each step a new hidden state is computed from the previous hidden state and the next input vector by a function $f$ parametrized by $\Theta_1$. The output at each step is computed from the hidden state using a function $g$ that is sometimes parametrized by $\Theta_2$. This allows the RNN to 'remember' important signals while scanning the context and to recognize signals spanning multiple words.
	
	\begin{equation}
	\label{eq1}
	\begin{aligned}
	& h_t=f_{\Theta_1}(h_{t-1}, v_t) \\
	& o_t=g_{\Theta_2}(h_t)
	\end{aligned}
	\end{equation}

	While an RNN unit can be used as-is in our model by feeding the last output vector $o_t$ directly into the classifier network, we have implemented an attention mechanism that allows the model to be aware of the candidate entity it is evaluating when computing an output vector. Equation \ref{eq2} details the equations governing the attention model.
	
	\begin{equation}
	\label{eq2}
	\begin{aligned}
	& a_t \in \mathbb{R}; a_t=r_{\Theta_3}(o_t, v_{candidate}) \\
	& a'_t  = \frac{1}{\sum_{i=1}^{t} \exp\{a_i\}} \exp \{a_t\} \\
	& o_{attn}=\sum_{i=1}^{t} a'_t o_t
	\end{aligned}
	\end{equation}
	
	The main component in equation \ref{eq2} is the function $r$, parametrized by $\Theta_3$, which computes an attention value at each step using $v_{candidate}$, the candidate entity embedding, as a control signal. We then use a softmax to squeeze the attention values such that $\sum_{i=1}^{t} a'_i = 1$ and compute the final output $o_{attn}$ as a weighted sum of all the output vectors of the RNN. This allows the attention mechanism to decide how much individual parts of the context are important when examining a specific candidate.
	
	\begin{equation}
	\label{eq3}
	r_{\Theta_3}(o_t, v_{candidate}) = Ao_t + Bv_{candidate} + b \\
	\end{equation}
	
	We parametrize our attention function $r$ as a single layer NN as shown in equation \ref{eq3}
	
	\subsection{\hl{Move this into experimental section}}
	We have implemented the entire model using the excellent Keras \cite{chollet2015} over Theano \cite{team2016theano} framework.
	
	\subsection{Training initial word and entity embeddings}
	
	Training our model implicitly trains its dictionaries of both word and entity embedding by error back-propagation. However, as will be shown in section \ref{experiments}, we have found using pre-trained embeddings to significantly improve model performance. To this end we have devised a Skip Gram with Negative Sampling (SGNS) \cite{mikolov2013distributed} based training procedure that simultaneously trains both word and entity vectors in the same embedded space. We note Yamada at el. \cite{yamada2016joint} have devised a somewhat similar method however our method trains much faster and requires only the textual content of Wikipedia pages while they require Wikipedia cross-references and category structure as well.
	
	We use the word2vecf library\footnote{Available at https://bitbucket.org/yoavgo/word2vecf} by Levy and Goldberg \cite{levy2014dependency} that is adapted from word2vec code and allows to train on a dataset made of $(word,context)$ pairs rather then a textual corpus in string format, as is done in the original word2vec. We exploit this to redefine $context$ as a context entity rather then a contextual word. 
	
	We do this by considering each page in Wikipedia to represent a unique entity, enumerated by the $pageid$ identifier in Wikipedia database and having a textual description (the page itself). For each word $\{word_i\}$ in the page we add the pair $(word_i,pageid)$ to our dataset. We however limit our vocabularies by ignoring both rare words that appear less then $20$ times and entities that have less then $20$ words in their description.
		
	As shown by Levy and Goldberg \cite{levy2014neural} training embeddings on this dataset using SGNS produces word and entity embedding that implicitly factorize the word-entity co-occurrence PPMI matrix. This matrix is closely related to the TFIDF word-entity matrix used by Gabrilovich and Markovitch \cite{gabrilovich2007computing} in Explicit Semantic Analysis and found to be useful in a wide array of NLP tasks. 
	
	For our experiments we trained embeddings of length 300 for 10 iterations over the dataset. We used default values for all other parameters in word2vec.
	
	\hl{-needs developing-}
	
	\hl{-show results of the analogies experiment we did indicating semantic structure for the WORD vectors-}
	
	\section{Web-Fragment based NED Dataset}
	We introduce a new large-scale NED dataset of web-fragments crawled from the web. Our dataset is derived from the Wikilinks dataset originally collected by Singh at el. \cite{singh12:wiki-links} for a cross-document co-reference task. cross-document co-reference entails clustering mentions referring to the same entity across a set of documents without consulting a predefined knowledge base of entities, and is many-a-time regarded as a downstream task for knowledge base population (KBP). Wikilinks was constructed by crawling the web and collecting hyperlinks linking to Wikipedia and the web context they appear in. The anchor texts act as mentions and the link targets in Wikipedia act as ground-truths. Wikilinks contains 40 million mentions covering 3 million entities and collected from over 10 million web pages.
	
	Despite the difference between a NED and a co-reference task, we believe the nature of Wikilinks makes it valuable to NED as well. This was recognized by Chisholm and Hachey \cite{chisholm2015entity} who examined collecting statistics from Wikilinks for a NED system. However we are interested in Wikilinks as a test case. 
	
	Wikilinks can be seen as a large-scale, naturally-occurring and crowd-sourced dataset where thousands of human annotators provide ground-truths for mentions of interest. Its web sourcing entails every kind of noise expected from automatically gathered web content, including many faulty, misleading and peculiar ground truth labels on the one hand, and on the other hand noisy, malformed and incoherent textual context for mentions. While noise in crowd-sourced data is arguably a necessary trade-off for quantity, we believe the contextual noise in particular represents an interesting test-case that supplements existing standard datasets such as CoNLL \cite{hoffart2011robust}, ACE and Wiki \cite{ratinov2011local} as these are all sourced from mostly coherent and well formed text such as news articles and Wikipedia pages. Wikilinks emphasizes utilizing strong and adaptive local disambiguation techniques, and marginalizes the utility of coherency based global approaches.
	
	\subsection{Preprocessing the Wikilinks dataset}
	
	We detail a number of filtering and processing steps used to adapt Wikilinks as a test dataset for NED. The original dataset exists in a number of formats, and we have chosen a version with only short local contexts\footnote{Available at http://www.iesl.cs.umass.edu/data/wiki-links} since it renders the size of the dataset a manageable 5Gb of compressed data (compared to 180Gb for the full texts).
	
	The first preprocessing step we took was resolving ground-truth links using a $7/4/2016$ dump of the Wikipedia database \footnote{Recent Wikipedia dumps are found at https://dumps.wikimedia.org/}. The same dump was consistently used throughout this research. We used the $page$ and $redirect$ tables for resolution and kept the database $pageid$ column as a unique identifier for Wikipedia pages (entities). We discarded mentions where the ground-truth could not be resolved, resulting in retention of over $97\%$ of the mentions. We then randomly permuted the order of mentions within the data and split it into train, evaluation and test set. We split the data $90\% / 10\% / 10\%$ respectively. Since websites might include duplicate or closely related content we did not assign mentions into splits on an individual basis but rather collected all origin domains and assigned each domain along with all mentions collected from it into a split collectively.
	
	The last and most important task was filtering the data. We collected all pairs of mention $m$ and entity $e$ appearing in the dataset and computed the following two statistics: how many times $m$ refers to $e$: $\#\{e|m\}$ and the conditional probability of $e$ given $m$: $p(e|m)=\#\{e|m\}/\sum_{e'}\#\{e'|m\}$. Examining these distributions revealed many mentions belong to two extremes: either they had very little ambiguity or had a number of candidate entities each appearing very few times. We have deemed the former to be unambiguous and not-interesting, and the latter to be suspected as noise with high probability. We therefore designed a procedure to filter both this cases: We retained only mentions for whom at least two ground-truth entities have $\#\{e|m\}\ge 10$ and $p(e|m)\ge0.1$. 
	
	This procedure aggressively filtered the dataset and we were left with $2.6M$ training, $300K$ test and $300K$ evaluation samples. We believe that doing so filters uninteresting cases while emitting a dataset that is large-scale yet manageable in size for research purposes. 
	
	We note that we have considered filtering $(m,e)$ pairs where $\#\{e|m\}\le 3$ since these are suspected as additional noise however decided against this procedure as it might filter out long tail entities, a case which was deemed interesting by the community.
	
	\section{Experiments} \label{experiments}
	
	\subsection{Training details}
	We now describe our setup for evaluating the proposed model for the task of NED. Following the motivation for disambiguating the Wikilinks data set, which was explained in section \refname{Web-Fragment based NED Dataset}, we trained our model on $2.6M$ mentions. This procedure was carried out with a 20-core CPU machine during XX days \hl{add description of the machine and timing}.
	
	Prior to the training we generated a basic counts/statistics file of mentions and entities from our corpus in a statistics file, for the creation of additional features and filtering mentions.  [What else?]. Specifically, it held a mention-count, an entity-count, a mention-links with the distribution over the possible candidates and a count for all words appearing in the context of a mention. 

	Due to framework limitations we have trained our RNNs with a fixed size left and right contexts. These where set a 20 word window to each side of the mention. When the context is shorter on either side, we pad it with a special $PAD$ symbol. We use a stop-word list to filter less informative stop-words.
	
	The input in the model was streamed as pairs of entity-candidate. Following Mikolov at el. \cite{mikolov2013distributed} we used a Negative Sampling approach where for each positive example the network is trained on, it is also trained on $k$ negative samples. In this framework, the context remains the same , while incorrect candidates are randomly sampled from all possible entities. We deviate from Mikolov , which picks negative examples according to their distribution in the training corpus, by uniform sampling from all possible entities . \hl{double check - In uniform sampling the ratio of positive to negative samples for each entity is proportional to the prior probability of the entity within the corpus, which bias the network toward ranking common entities higher.}
	
	The optimization of the model was carried out using standard back propagation and AdaGrad optimizer\cite{duchi2011adaptive}. We allowed the error to propagate through all parts of the network and fine tune all trainable parameters, including fine-tuning the word and entity embeddings themselves. The scale of the data enabled us to run only 1 epoch over the data.
    
    CoNLL 2003 NER data is a an evaluation that is commonly used in research for benchmarking NED solutions \cite{Globerson2016,Hachey2013,Yamada2016,Pershina2015}. This has driven us to hold additional models, trained on $18505$ non-NIL mentions from the CoNLLs training set, and evaluated on test-b set with $4485$ cases. 

	{\bf CoNLL}
	The CoNLL corpus is a public, free and annotated dataset, which is compromised out of a relatively large amount of news articles written in 1996 by the Reuters newswire. It shares $1393$ documents from two time periods of overall $12$ days, which are split into train, development and test set according to $68-15-17$\%. The performance on the corpus is usually measured using micro accuracy (p@1), which is a the average over the label of the mentions, having the golden-sense in their candidate set. While disregarding the performance of the candidate generation process, this measure can be delusive when the recall of the candidate generation is low. Therefore, it is common to use candidate generation resources, such as YAGO \cite{hoffart2011robust} or PPRforNED \cite{Pershina2015}, with over $99$\% candidate recall for better comparability of the results. Another broadly used measure is the macro accuracy, which averages the precision of all documents.Typical and recent p@1 results of local disambiguation approaches on the test-b dataset have ranged between $~80$\% to $~89$\% (see table \ref{}), when very recently the $90$\% barrier was broken by Yamada et al.\cite{Yamada2016}. Results on CoNLL appear sometimes in literature under the AIDA system, which is an online disambiguation system offered by Hoffart that contains the CoNLL 2003 NER task.
	
	\subsection{Learning to rank}
	We had two kinds of models trained on the CoNLL corpus. Initially, we used the pure Attentional RNN architecture, trained over $20$\% \hl{is this right?} of the intralinks corpus with fine-tuning. Because of the scale of the data we used only one epoch.
	Inspired by Yamada's approach, we extended our model to be part of a parent Gradient Boosting Regression Tree (GBRT) classifier that is known for demonstrating neat performance in ranking applications \cite{friedman2001greedy}. For every data fragment, we took the preceding model's prediction, which is the probability that a candidate suits the input mention given the context, and fed it into a feature vector. In this manner we generated an input vector for every pair of mention-candidate from the CoNLL training set and let the model iterate over the entire data. Practically, we used sklearn's GradientBoostingClassifier implementation \cite{pedregosa2011scikit} with parameters similar to those reported by Yamada. We trained with the logistic-regression deviance and set the learning rate, number of estimator and maximum depth of a tree to $0.02$, $10000$ and $4$, respectively. For the CoNLL experiments we also added Yamada's base and string similarity features into the input vector. 
	We find it important to note that in contrast to many other studies, we didn't focus on surpassing current state of the art results on the CoNLL task, as it is a precisely edited and specific newspaper corpus with stories covering merely two weeks of reports \cite{Sang2003}. 

	\subsection{Count and string based features \hl{necessary??}}
	Besides of embeddings of the context and entities' the model's input consisted several more features. We implemented the base features that were introduced by Yamada using statistics from Wikipedia that were computed prior to the training. In this manner we obtained the conditional and marginal entity prior, $P(e|m)$ and $P(e)$, which are normalized measures giving the number of times entity $e$ was linked to a specific or to any mention $m$ in the dataset. Information about the source document was captured by the max prior feature that is takes the maximum prior probability of 
	
	\subsection{Comparison with other methods}
	We use our the suggested Wikilinks corpus from section \refname{Web-Fragment based NED Dataset} to compare the results of our model with two state of the art and well established algorithms.
	
	\begin{itemize} 
		\item  \textbf{Yamada et al.} \cite{Yamada2016} have published a recent paper that introduces the highest score of precision@1 on CoNLL test-b. For this reason we used this work as a principle guideline for the evaluation over the CoNLL corpus. The paper describes an embedding method that learns the relatedness of entities and jointly maps entities and context to the same vector space using the skip-gram model. The distance between these embeddings is then computed to form several features  of an input vector used for point-wise training an GBRT model. Full Yamada refers to the complete algorithm, including coherence and the two-step approach, while Yamada (no coherence) is the partial version, which excludes the former steps.
		
		\item \textbf{Cheng et al.} \cite{Cheng2013} addressed the Disambiguation to Wikipedia, or the "Wikificaiton" task, with a combination of local and global solutions. In this model, mentions are disambiguated locally by generating for each of them a ranked list of candidates using the GLOW Wikification system offered by Ratinov et al. \cite{Ratinov2011}. We compare our results to the ranking step of the algorithm, were a linear Ranking Support Vector Machine is trained over a set of local and global features to return the list of candidate sorted by their likelihood. We compared our results to the Ranker accuracy metric , which is relevant an evaluation metric equivalent to the p@1 score.
	\end{itemize}
	
	As a baseline we took the standard Most Probable Sense (MPS) prediction, which corresponds to the $\arg\max_{e\in{{E}}}{P(e|m)}$, where $E$ is the group of all possible entities.
	In addition to the former models we also present performance on CoNLL of the following papers - Lazic et al. \cite{Lazic2015}, Francis-Landau et al. \cite{Francis-Landau2016}, He et al. \cite{He2013}, Hoffart et al. \cite{hoffart2011robust} and Chisholm et al. \cite{Chisholm2015} ,as they are strong local approaches which demonstrate relative good results.
	
	\subsection{Results}
	
	The micro and macro accuracy scores on CoNLL test-b are displayed in table \ref{tab:a}. We used the \textit{PPRforNED} dataset \cite{Pershina2015} for extracting the candidates for each mention, as it yielded $99.7$\% candidate-generation recall and provided better results compared to YAGO. 
	
	
	\begin{table}[h]
		\begin{center}
			\begin{tabular}{|p{3.5cm}| p{1.5cm} p{1.5cm}|}
				\hline \multicolumn{3}{|c|}{CoNLL test-b} \\
				\hline Model & Micro     accuracy & Macro     accuracy \\ \hline
				\bf ARNN  & \bf 84.2 & \bf 85 \\
				\bf GBRT: Base + ARNN features & \bf 85.6 & \bf 88 \\
				Yamada et al. (no coherence) & 90.9 & 92.4 \\
				Lazic et al. & 86.4 & - \\
				Francis-Landau et al. & 85.5 & - \\
				He et al. & 84.82 & 83.37 \\	
				Hoffart et al. & 79.57 & 80.71 \\
				Chisholm et al. & 88.7 & - \\			
				Baseline (MPS) & 77 & 77 \\
				\hline
			\end{tabular}
		\end{center}
		\caption{\label{tab:a} Evaluation on CoNLL. Bold font denotes the models offered in this study}
	\end{table}

	In the following table we see performance of models on the Wikilinks test set. With this corpus we report on $97.3$\% candidate generation recall, having in average 13.5 candidates per mention.
	 
	\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c| p{1.5cm}|}
			\hline \multicolumn{2}{|c|}{Wikilinks test set} \\
			\hline \bf Model & \bf Micro     accuracy  \\ \hline
			ARNN  & 84.2\\
			GBRT: Base + ARNN features & 66.8 \\
			Yamada (no coherence) & -  \\
			Update results & ... \\
			Baseline (MPS) & 55.9 \\
			\hline
		\end{tabular}
	\end{center}
	\caption{\label{tab:b} Evaluation on Web-Fragment data (Wikilinks)}
	\end{table}

	** add insights regarding the results and comparison
	\subsection{Methods sensitivity/study}
	
	elaborate on: \newline
	** ESA embedding initialization \newline
	** intra vs. wikilinks training \newline
	** cost function (max-margin vs. binary crossentropy) \newline
	** downsampling
	
	\section{Conclusions}
	
	\bibliographystyle{acl2016}
	\bibliography{_our_submission,_noam_bib}
	
\end{document}
