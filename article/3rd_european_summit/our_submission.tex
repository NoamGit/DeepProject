\documentclass[format=acmsmall, review=true, screen=true]{acmart}

\usepackage{booktabs} % For formal tables

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
%\acmArticleSeq{9}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% Document starts
\begin{document}
% Title portion. Note the short title for running heads 
\title[Named Entity Disambiguation for Noisy Text]{Named Entity Disambiguation for Noisy Text}  
\author{Yotam Eshel}
\affiliation{%
  \institution{Technion}
  \country{Israel}}
\author{Noam Cohen}
\affiliation{%
	\institution{Technion}
	\country{Israel}}
\author{Kira Radinsky}
\affiliation{%
	\institution{eBay}
	\country{Israel}}
\author{Shaul Markovitch}
\affiliation{%
	\institution{Technion}
	\country{Israel}}
\author{Ikuya Yamada}
\affiliation{%
	\institution{Studio Ousia}
	\country{Japan}}
\author{Omer Levi}
\affiliation{%
	\institution{University of Washington}
	\country{USA}}

\begin{abstract}
	Named Entity Disambiguation (NED) is the task of linking mentions of entities in text to a given knowledge base, such as Freebase or Wikipedia. 
	NED is a key component in Entity Linking (EL) systems, focusing on the disambiguation task itself, independently from the tasks of Named Entity Recognition (detecting mention bounds) and Candidate Generation (retrieving the set of potential candidate entities). NED has been recognized as an important component in NLP tasks such as semantic parsing \cite{berant2013semantic}.
	
	Current research on NED is mostly driven by a number of standard datasets, such as CoNLL-YAGO \cite{hoffart2011robust}, TAC KBP \cite{ji2010overview} and ACE \cite{bentivogli2010extending}. These datasets are based on news corpora and Wikipedia, which are naturally coherent, well-structured, and rich in context. Global disambiguation models \cite{guo2014entity,pershina2015personalized,Globerson2016} leverage this coherency by jointly disambiguating all the mentions in a single document. However, domains such as web-page fragments, social media, or search queries, are often short, noisy, and less coherent; such domains lack the necessary contextual information for global methods to pay off, and present a more challenging setting in general.
	
	In this work, we investigate the task of NED in a setting where only \textit{local} and \textit{noisy} context is available. In particular, we create a dataset of 3.2M short text fragments extracted from web pages, each containing a mention of a named entity. Our dataset is far larger than previously collected datasets, and contains 18K unique mentions linking to over 100K unique entities. We have empirically found it to be noisier and more challenging than existing datasets. For example:
	\begin{quote}
		``I had no choice but to experiment with other indoor games. I was born in Atlantic City so the obvious next choice was \textbf{Monopoly}. I played until I became a successful Captain of Industry.''
	\end{quote}
	This short fragment is considerably less structured and with a more personal tone than a typical news article. It references the entity \textit{Monopoly\_(Game)}, however expressions such as ``experiment'' and ``Industry'' can distract a naive disambiguation model because they are also related the much more common entity \textit{Monopoly} (economics term). Some sense of local semantics must be considered in order to separate the useful signals (e.g. ``indoor games'', ``played'') from the noisy ones.
	
	We therefore propose a new model that leverages local contextual information to disambiguate entities. Our neural approach (based on RNNs with attention) leverages the vast amount of training data in WikilinksNED to learn representations for entity and context, allowing it to extract signals from noisy and unexpected context patterns. 
	
	While convolutional neural networks \cite{sun2015modeling,francis2016capturing} and probabilistic attention \cite{Lazic2015} have been applied to the task, this is the first model to use RNNs and a neural attention model for NED. RNNs account for the sequential nature of textual context while the attention model is applied to reduce the impact of noise in the text. 
	
	Our experiments show that our model significantly outperforms existing state-of-the-art NED algorithms on WikilinksNED, suggesting that RNNs with attention are able to model short and noisy context better than current approaches. In addition, we evaluate our algorithm on CoNLL-YAGO \cite{hoffart2011robust}, a dataset of annotated news articles. We use a simple domain adaptation technique since CoNLL-YAGO lacks a large enough training set for our model, and achieve comparable results to other state-of-the-art methods. These experiments highlight the difference between the two datasets, indicating that our NED benchmark is substantially more challenging.
	
	%We analyze our results both quantitatively and qualitatively, and conclude with a number of possible directions for future work, such as handling of semantically highly relayed entities (e.g. a book and a movie of the same narrative).
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%


%
% End generated code
%


\keywords{Entity Linking, Entity Disambiguation}

\maketitle

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. Zhou et al.}

\bibliography{_our_submission}
\bibliographystyle{ACM-Reference-Format}

\end{document}

