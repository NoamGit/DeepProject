Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Singh2012,
author = {Singh, Sameer and Subramanya, Amarnag and Pereira, Fernando and McCallum, Andrew},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/UM-CS-2012-015.pdf:pdf},
journal = {{\ldots} of Massachusetts, Amherst {\ldots}},
pages = {1--14},
title = {{Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Wikilinks+:+A+Large-scale+Cross-Document+Coreference+Corpus+Labeled+via+Links+to+Wikipedia{\#}0},
year = {2012}
}
@article{Mikolov,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
eprint = {arXiv:1301.3781v3},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/efficent representetion vectot space -mikolov.pdf:pdf},
pages = {1--12},
title = {{Vector Space}}
}
@article{Shen2015,
abstract = {The large number of potential applications from bridging Web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.},
author = {Shen, Wei and Wang, Jianyong and Han, Jiawei},
doi = {10.1109/TKDE.2014.2327028},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen, Wang, Han - 2015 - Entity linking with a knowledge base Issues, techniques, and solutions.pdf:pdf},
isbn = {1041-4347 VO - 27},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Entity linking,entity disambiguation,knowledge base},
number = {2},
pages = {443--460},
pmid = {1609378},
title = {{Entity linking with a knowledge base: Issues, techniques, and solutions}},
volume = {27},
year = {2015}
}
@article{Giuliano2009,
abstract = {We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner. In particular, we use a combination of basic kernel functions to independently estimate syntagmatic and domain similarity, building a set of word-expert classifiers that share a common domain model acquired from a large corpus of unlabeled data. The results show that the proposed approach achieves state-of-the-art performance on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although it uses a considerably smaller number of training examples than other methods.},
author = {Giuliano, Claudio and Gliozzo, Alfio Massimiliano and Strapparava, Carlo},
doi = {10.1162/coli.2009.35.4.35407},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/Kernel{\_}Methods{\_}for{\_}Minimally{\_}Supervised{\_}WSD.pdf:pdf},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {4},
pages = {513--528},
title = {{Kernel Methods for Minimally Supervised WSD}},
volume = {35},
year = {2009}
}
@article{Cheng2013,
abstract = {Wikification, commonly referred to as Disam- biguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific correspondingWikipedia pages. Previous ap- proaches to D2W focused on the use of lo- cal and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these meth- ods fail (often, embarrassingly), when some level of text understanding is needed to sup- port Wikification. In this paper we introduce a novel approach to Wikification by incorpo- rating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Lin- ear Programming (ILP) formulation of Wik- ification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant im- provements in bothWikification and the TAC Entity Linking task.},
author = {Cheng, Xiao and Roth, Dan},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Roth - 2013 - Relational Inference for Wikification(2).pdf:pdf;:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Roth - 2013 - Relational Inference for Wikification(3).pdf:pdf},
isbn = {9781937284978},
journal = {Empirical Methods in Natural Language Processing},
number = {October},
pages = {1787--1796},
title = {{Relational Inference for Wikification}},
url = {http://aclweb.org/anthology/D/D13/D13-1184.pdf},
year = {2013}
}
@article{Globerson2016,
abstract = {Entity resolution is the task of linking each mention of an entity in text to the cor-responding record in a knowledge base (KB). Coherence models for entity resolu-tion encourage all referring expressions in a document to resolve to entities that are related in the KB. We explore attention-like mechanisms for coherence, where the evidence for each candidate is based on a small set of strong relations, rather than relations to all other entities in the doc-ument. The rationale is that document-wide support may simply not exist for non-salient entities, or entities not densely connected in the KB. Our proposed sys-tem outperforms state-of-the-art systems on the CoNLL 2003, TAC KBP 2010, 2011 and 2012 tasks.},
author = {Globerson, Amir and Lazic, Nevena and Chakrabarti, Soumen and Subramanya, Amarnag and Ringgaard, Michael and Pereira, Fernando},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/P16-1059.pdf:pdf},
journal = {Acl},
pages = {621--631},
title = {{Collective Entity Resolution with Multi-Focal Attention}},
year = {2016}
}
@article{Trask2015a,
abstract = {Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or "senses". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8{\%} average error reduction in unlabeled attachment scores across 6 languages.},
archivePrefix = {arXiv},
arxivId = {1511.06388},
author = {Trask, Andrew and Michalak, Phil and Liu, John},
eprint = {1511.06388},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/1511.06388v1.pdf:pdf},
journal = {arXiv preprint arXiv:1511.06388},
pages = {1--9},
title = {{sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings}},
url = {http://arxiv.org/abs/1511.06388},
year = {2015}
}
@article{Yuan2016,
abstract = {Determining the intended sense of words in text -- word sense disambiguation (WSD) -- is a long-standing problem in natural language processing. In this paper, we present WSD algorithms which use neural network language models to achieve state-of-the-art precision. Each of these methods learns to disambiguate word senses using only a set of word senses, a few example sentences for each sense taken from a licensed lexicon, and a large unlabeled text corpus. We classify based on cosine similarity of vectors derived from the contexts in unlabeled query and labeled example sentences. We demonstrate state-of-the-art results when using the WordNet sense inventory, and significantly better than baseline performance using the New Oxford American Dictionary inventory. The best performance was achieved by combining an LSTM language model with graph label propagation.},
archivePrefix = {arXiv},
arxivId = {1603.07012},
author = {Yuan, Dayu and Doherty, Ryan and Richardson, Julian and Evans, Colin and Altendorf, Eric},
eprint = {1603.07012},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/1603.07012v1.pdf:pdf},
journal = {Arxiv},
title = {{Word Sense Disambiguation with Neural Language Models}},
url = {http://arxiv.org/abs/1603.07012},
year = {2016}
}
@article{Huang2014,
abstract = {Wikification for tweets aims to automat- ically identify each concept mention in a tweet and link it to a concept referent in a knowledge base (e.g., Wikipedia). Due to the shortness of a tweet, a collective inference model incorporating global ev- idence from multiple mentions and con- cepts is more appropriate than a non- collecitve approach which links each men- tion at a time. In addition, it is chal- lenging to generate sufficient high quality labeled data for supervised models with low cost. To tackle these challenges, we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multi- ple tweets through three fine-grained re- lations. In order to identify semantically- related mentions for collective inference, we detect meta path-based semantic rela- tions through social networks. Compared to the state-of-the-art supervised model trained from 100{\%} labeled data, our pro- posed approach achieves comparable per- formance with 31{\%} labeled data and ob- tains 5{\%} absolute F1 gain with 50{\%} la- beled data.},
author = {Huang, Hongzhao and Cao, Yunbo and Huang, Xiaojiang and Ji, Heng and Lin, Chin-yew},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/P14-1036.pdf:pdf},
isbn = {9781937284725},
journal = {Acl},
pages = {380--390},
title = {{Collective Tweet Wikification based on Semi-supervised Graph Regularization}},
year = {2014}
}

@article{Moro2015,
author = {Moro, Andrea and Navigli, Roberto and Informatica, Dipartimento and Elena, Viale Regina},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/SemEval049.pdf:pdf},
journal = {SemEval2015},
number = {SemEval},
pages = {288--297},
title = {{Multilingual All-Words Sense Disambiguation and Entity Linking}},
year = {2015}
}
@article{Lazic2015,
abstract = {We present Plato, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features, and supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 10{\^{}}7 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets.},
author = {Lazic, Nevena and Subramanya, Amarnag and Ringgaard, Michael and Pereira, Fernando},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazic et al. - 2015 - Plato A Selective Context Model for Entity Resolution.pdf:pdf;:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazic et al. - 2015 - Plato A Selective Context Model for Entity Resolution(2).pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {503--515},
title = {{Plato: A Selective Context Model for Entity Resolution}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/637},
volume = {3},
year = {2015}
}
@article{Mihalcea2007,
abstract = {This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disambiguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations.},
author = {Mihalcea, Rada and Csomai, Andras},
doi = {10.1145/1321440.1321475},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/mihalcea.cikm07.pdf:pdf},
isbn = {9781595938039},
issn = {15411672},
journal = {Proceedings of the sixteenth ACM conference on Conference on information and knowledge management},
keywords = {keyword extraction,semantic annotation,wikipedia,word sense disambiguation},
pages = {233--242},
title = {{Wikify!: linking documents to encyclopedic knowledge}},
url = {http://doi.acm.org/10.1145/1321440.1321475},
year = {2007}
}
@article{Singh2011,
abstract = {Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38{\%}) on this large dataset, demonstrating the scalability of our approach.},
author = {Singh, Sameer and Subramanya, Amarnag and Pereira, Fernando and McCallum, Andrew},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/37560.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
pages = {793--803},
title = {{Large-scale cross-document coreference using distributed inference and hierarchical models}},
url = {http://dl.acm.org/citation.cfm?id=2002472.2002573$\backslash$nhttp://dl.acm.org/ft{\_}gateway.cfm?id=2002573{\&}type=pdf$\backslash$nhttps://www.aclweb.org/anthology/P/P11/},
year = {2011}
}
@article{Kim2015,
abstract = {While most previous work on Wikification has focused on written texts, this paper presents a Wikification approach for spo-ken dialogues. A set of analyzers are pro-posed to learn dialogue-specific properties along with domain knowledge of conver-sations from Wikipedia. Then, the an-alyzed properties are used as constraints for generating candidates, and the candi-dates are ranked to find the appropriate links. The experimental results show that our proposed approach can significantly improve the performances of the task in human-human dialogues.},
author = {Kim, Seokhwan and Banchs, Rafael E and Li, Haizhou},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Banchs, Li - 2015 - Wikification of Concept Mentions within Spoken Dialogues Using Domain Constraints from Wikipedia.pdf:pdf},
isbn = {9781941643327},
number = {September},
pages = {2225--2229},
title = {{Wikification of Concept Mentions within Spoken Dialogues Using Domain Constraints from Wikipedia}},
year = {2015}
}
@article{Levy,
author = {Levy, Omer},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy - Unknown - Improving Distributional Similarity with Lessons Learned from Word Embeddings.pdf:pdf},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}}
}
@article{Chisholm2015,
abstract = {Entity disambiguation with Wikipedia relies on structured information from redirect pages, article text, inter-article links, and categories. We explore whether web links can replace a curated encyclopaedia, obtaining entity prior, name, context, and coherence models from a corpus of web pages with links to Wiki- pedia. Experiments compare web link models to Wikipedia models on well-known CoNLL and TAC data sets. Results show that using 34 million web links approachesWikipedia performance. Combin- ing web link and Wikipedia models produces the best-known disambiguation accuracy of 88.7 on standard newswire test data. 1},
author = {Chisholm, Andrew and Hachey, Ben},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/Chisholm 2015.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
number = {0},
pages = {145--156},
title = {{Entity Disambiguation with Web Links}},
volume = {3},
year = {2015}
}
@article{Palangi2016,
abstract = {This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms it for web document retrieval task.},
archivePrefix = {arXiv},
arxivId = {1502.06922},
author = {Palangi, Hamid and Deng, Li and Shen, Yelong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Song, Xinying and Ward, Rabab},
doi = {10.1109/TASLP.2016.2520371},
eprint = {1502.06922},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/1502.06922.pdf:pdf},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
number = {4},
pages = {694--707},
title = {{Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval}},
url = {http://arxiv.org/abs/1502.06922$\backslash$nhttp://www.arxiv.org/pdf/1502.06922.pdf$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7389336$\backslash$nhttp://arxiv.org/abs/1502.06922},
volume = {24},
year = {2016}
}
@article{Mikolov2010c,
abstract = {基于RNN模型的语言模型，详细可参考作者的博士论文。周期神经网络。但是上下文，也没用取全部的，只取到了前5个。},
author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
file = {:C$\backslash$:/Users/Noam/Downloads/papers/mikolov{\_}interspeech2010{\_}IS100722.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1045--1048},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}

@article{Melamud2014,
author = {Melamud, Oren and Goldberger, Jacob},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/context2vec{\_}conll16.pdf:pdf},
title = {{Learning Generic Context Embedding with Bidirectional LSTM}},
year = {2014}
}
@article{Trask2015,
abstract = {Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or "senses". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8{\%} average error reduction in unlabeled attachment scores across 6 languages.},
archivePrefix = {arXiv},
arxivId = {1511.06388},
author = {Trask, Andrew and Michalak, Phil and Liu, John},
eprint = {1511.06388},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/1511.06388v1.pdf:pdf;:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/1511.06388v1.pdf:pdf},
journal = {arXiv preprint arXiv:1511.06388},
pages = {1--9},
title = {{sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings}},
url = {http://arxiv.org/abs/1511.06388},
year = {2015}
}
@article{Ponzetto2010,
abstract = {One of the main obstacles to high- performance Word Sense Disambigua- tion (WSD) is the knowledge acquisi- tion bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of seman- tic relations from an encyclopedic re- source, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, sim- ple knowledge-lean disambiguation algo- rithms compete with state-of-the-art su- pervisedWSDsystems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.},
author = {Ponzetto, Simone Paolo and Navigli, Roberto and Informatica, Dipartimento},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponzetto, Navigli, Informatica - 2010 - Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems.pdf:pdf},
isbn = {9781617388088},
journal = {Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics},
number = {July},
pages = {1522--1531},
title = {{Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems}},
year = {2010}
}
@article{Hachey2013,
abstract = {Named Entity Linking (nel) grounds entity mentions to their corresponding node in a Knowledge Base (kb). Recently, a number of systems have been proposed for linking entity mentions in text to Wikipedia pages. Such systems typically search for candidate entities and then disambiguate them, returning either the best candidate or nil. However, comparison has focused on disambiguation accuracy, making it difficult to determine how search impacts performance. Furthermore, important approaches from the literature have not been systematically compared on standard data sets. We reimplement three seminal nel systems and present a detailed evaluation of search strategies. Our experiments find that coreference and acronym handling lead to substantial improvement, and search strategies account for much of the variation between systems. This is an interesting finding, because these aspects of the problem have often been neglected in the literature, which has focused largely on complex candidate ranking algorithms. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Hachey, Ben and Radford, Will and Nothman, Joel and Honnibal, Matthew and Curran, James R.},
doi = {10.1016/j.artint.2012.04.005},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/hachey-aij12-evaluating.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Disambiguation,Information extraction,Named Entity Linking,Semi-structured resources,Wikipedia},
pages = {130--150},
pmid = {1873413},
title = {{Evaluating entity linking with wikipedia}},
volume = {194},
year = {2013}
}
@article{Moro2014,
author = {Moro, Andrea and Raganato, Alessandro and Navigli, Roberto},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/TACL{\_}2014{\_}Babelfy.pdf:pdf},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
pages = {231--244},
title = {{Entity Linking meets Word Sense Disambiguation: a Unified Approach}},
volume = {2},
year = {2014}
}
@article{Agirre2007,
author = {Agirre, Eneko and Edmonds, Philip},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agirre, Edmonds - 2007 - Word Sense Disambiguation Algorithms and Applications.pdf:pdf},
isbn = {1402068700, 9781402068706},
title = {{Word Sense Disambiguation Algorithms and Applications}},
year = {2007}
}
@article{Wsd,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781},
author = {Wsd, M S H},
eprint = {arXiv:1301.3781},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wsd - Unknown - httpwww.cs.technion.ac.il{\~{}}gabrresourcesdatawordsim353 httpscode.google.comarchivepword2vec 1.pdf:pdf},
number = {Id 200140549},
pages = {2--3},
title = {http://www.cs.technion.ac.il/{\~{}}gabr/resources/data/wordsim353/ https://code.google.com/archive/p/word2vec/ 1}
}
@article{Levya,
author = {Levy, Omer},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy - Unknown - Neural Word Embedding as Implicit Matrix Factorization.pdf:pdf},
pages = {1--9},
title = {{Neural Word Embedding as Implicit Matrix Factorization}}
}

@article{Method2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3722v1},
author = {Method, Negative-sampling Word-embedding and Goldberg, Yoav and Levy, Omer and Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {arXiv:1402.3722v1},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Method et al. - 2014 - arXiv 1402 . 3722v1 cs . CL 15 Feb 2014 word2vec Explained Deriving Mikolov et al .' s.pdf:pdf},
number = {2},
pages = {1--5},
title = {{arXiv : 1402 . 3722v1 [ cs . CL ] 15 Feb 2014 word2vec Explained : Deriving Mikolov et al .' s}},
year = {2014}
}
@article{Munich2004c,
author = {Graves, Alex and Schmidhuber, Jurgen},
doi = {10.1016/j.neunet.2005.06.042},
file = {:C$\backslash$:/Users/Noam/Downloads/papers/nn{\_}2005.pdf:pdf},
issn = {08936080},
title = {{Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures}},
year = {2004}
}
@article{Pershina2015,
abstract = {``````````````````````````````````````````````````````````````````````````````````````````````````````````````````qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq},
author = {Pershina, Maria},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/N15-1026.pdf:pdf},
isbn = {9781941643495},
number = {Section 4},
pages = {238--243},
title = {{Personalized Page Rank for Named Entity Disambiguation}},
year = {2015}
}
@article{Sang2003,
abstract = {We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
archivePrefix = {arXiv},
arxivId = {cs/0306050},
author = {Sang, Erik F. Tjong Kim and {De Meulder}, Fien},
doi = {10.3115/1119176.1119195},
eprint = {0306050},
file = {:C$\backslash$:/Users/Noam/Downloads/papers/W03-0419.pdf:pdf},
journal = {CONLL '03 Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003},
pages = {142--147},
primaryClass = {cs},
title = {{Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition}},
url = {http://arxiv.org/abs/cs/0306050},
volume = {4},
year = {2003}
}
@article{Acl2016,
author = {Acl, Anonymous},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/anders.pdf:pdf},
pages = {1--10},
title = {{Simple Count-Based Cross-Lingual Word Embeddings}},
year = {2016}
}
@article{Levyc,
author = {Levy, Omer and Goldberg, Yoav},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Goldberg - Unknown - Linguistic Regularities in Sparse and Explicit Word Representations.pdf:pdf},
title = {{Linguistic Regularities in Sparse and Explicit Word Representations}}
}
@article{Milne2008,
abstract = {This paper describes how to automatically cross-reference documents with Wikipedia: the largest knowledge base ever known. It explains how machine learning can be used to identify significant terms within unstructured text, and enrich it with links to the appropriate Wikipedia articles. The resulting link detector and disambiguator performs very well, with recall and precision of almost 75{\%}. This performance is constant whether the system is evaluated on Wikipedia articles or "real world" documents. This work has implications far beyond enriching documents with explanatory links. It can provide structured knowledge about any unstructured fragment of text. Any task that is currently addressed with bags of words - indexing, clustering, retrieval, and summarization to name a few - could use the techniques described here to draw on a vast network of concepts and semantics.},
author = {Milne, David and Witten, Ian H.},
doi = {10.1145/1458082.1458150},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wikification and supervised wsd/2008 - Learning To Link With Wikipedia.pdf:pdf},
isbn = {9781595939913},
issn = {978-1-59593-991-3},
journal = {Proceeding of the 17th ACM Conference on Information and Knowledge Management (CIKM '08)},
keywords = {data mining,semantic annotation,wikipedia,word sense,word sense disambiguation},
pages = {509--518},
title = {{Learning to link with Wikipedia}},
url = {http://dl.acm.org/citation.cfm?id=1458082.1458150},
year = {2008}
}
@article{Melamud2014a,
author = {Melamud, Oren and Goldberger, Jacob},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/context2vec{\_}conll16.pdf:pdf;:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/context2vec{\_}conll16.pdf:pdf},
title = {{Learning Generic Context Embedding with Bidirectional LSTM}},
year = {2014}
}
@article{Gal2015,
abstract = {A long strand of empirical research has claimed that dropout cannot be applied between the recurrent connections of a recurrent neural network (RNN). The reasoning has been that the noise hinders the network's ability to model sequences, and instead should be applied to the RNN's inputs and outputs alone. But dropout is a vital tool for regularisation, and without dropout in recurrent layers our models overfit quickly. In this paper we show that a recently developed theoretical framework, casting dropout as approximate Bayesian inference, can give us mathematically grounded tools to apply dropout within the recurrent layers. We apply our new dropout technique in long short-term memory (LSTM) networks and show that the new approach significantly outperforms existing techniques.},
archivePrefix = {arXiv},
arxivId = {1512.05287},
author = {Gal, Yarin},
eprint = {1512.05287},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/1512.05287v3.pdf:pdf},
title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.05287},
year = {2015}
}
@article{Gabrilovich2006,
author = {Gabrilovich, Evgeniy and Markovitch, Shaul},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gabrilovich, Markovitch - 2006 - Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.pdf:pdf},
keywords = {learning,natural language processing,web mining},
pages = {1606--1611},
title = {{Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis}},
year = {2006}
}
@article{Levyb,
author = {Levy, Omer},
file = {:C$\backslash$:/Users/Noam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy - Unknown - Neural Word Embedding as Implicit Matrix Factorization(2).pdf:pdf},
pages = {1--9},
title = {{Neural Word Embedding as Implicit Matrix Factorization}}
}
@article{He2013,
author = {He, Zhengyan and Liu, Shujie and Li, Mu and Zhou, Ming and Zhang, Longkai and Wang, Houfeng},
file = {:C$\backslash$:/Users/Noam/Dropbox/DeepESA/literature/wsd/Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.pdf:pdf},
pages = {30--34},
title = {{Learning Entity Representation for Entity Disambiguation}},
year = {2013}
}
